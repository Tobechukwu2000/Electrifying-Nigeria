{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Models: ANN, ELM, SVR, KRR, RF, XGB\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#from skelm import ELMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#import optuna\n",
    "import shap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = pd.read_excel(\"data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def fixed_randomization(data):\n",
    "    np.random.seed(42) \n",
    "    rand_data = data.sample(frac=1).reset_index(drop=True)\n",
    "    return rand_data\n",
    "\n",
    "rand_data = fixed_randomization(rawdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data_unscale = rand_data[rand_data.columns[0:15]]\n",
    "Y_data_unscale = rand_data[rand_data.columns[15:20]]\n",
    "\n",
    "Y_data = Y_data_unscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = StandardScaler()\n",
    "MMS = MinMaxScaler()\n",
    "X_data = SS.fit_transform(X_data_unscale)\n",
    "X_data = pd.DataFrame(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data.columns = X_data_unscale.columns\n",
    "X_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_data = Y_data[Y_data.columns[0:1]]\n",
    "y2_data = Y_data[Y_data.columns[1:2]]\n",
    "y3_data = Y_data[Y_data.columns[2:3]]\n",
    "y4_data = Y_data[Y_data.columns[3:4]]\n",
    "y5_data = Y_data[Y_data.columns[4:5]]\n",
    "\n",
    "\n",
    "y1_train = Y_train[Y_train.columns[0:1]]\n",
    "y2_train = Y_train[Y_train.columns[1:2]]\n",
    "y3_train = Y_train[Y_train.columns[2:3]]\n",
    "y4_train = Y_train[Y_train.columns[3:4]]\n",
    "y5_train = Y_train[Y_train.columns[4:5]]\n",
    "\n",
    "\n",
    "y1_test = Y_test[Y_test.columns[0:1]]\n",
    "y2_test = Y_test[Y_test.columns[1:2]]\n",
    "y3_test = Y_test[Y_test.columns[2:3]]\n",
    "y4_test = Y_test[Y_test.columns[3:4]]\n",
    "y5_test = Y_test[Y_test.columns[4:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Extract values from DataFrames\n",
    "    y_true_values = y_true.values.flatten()\n",
    "    y_pred_values = y_pred.values.flatten()\n",
    "    \n",
    "    # R2 score\n",
    "    r2 = r2_score(y_true_values, y_pred_values)\n",
    "    r2 = round(r2, 4)\n",
    "    \n",
    "    # Mean Squared Error (MSE)\n",
    "    mse = mean_squared_error(y_true_values, y_pred_values)\n",
    "    mse = round(mse, 4)\n",
    "    \n",
    "    # Root Mean Squared Error (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    rmse = round(rmse, 4)\n",
    "\n",
    "    # Average Absolute Deviation (AAD)\n",
    "    aad = np.mean(np.abs(y_true_values - y_pred_values))\n",
    "    aad = round(aad, 4)\n",
    "    \n",
    "    # Squared Error Percentage (SEP)\n",
    "    sep = np.mean(((y_true_values - y_pred_values) / y_true_values)**2) * 100\n",
    "    sep = round(sep, 4)\n",
    "    \n",
    "    # Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(y_true_values, y_pred_values)\n",
    "    mae = round(mae, 4)\n",
    "    \n",
    "    '''print('r2: ', r2)\n",
    "    print('mse: ', mse)\n",
    "    print('rmse: ', rmse)\n",
    "    print('aad: ', aad)\n",
    "    print('sep: ', sep)\n",
    "    print('mae: ', mae)'''   \n",
    "\n",
    "    print(r2)\n",
    "    print(mse)\n",
    "    print(rmse)\n",
    "    print(aad)\n",
    "    print(sep)\n",
    "    print(mae) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_data = pd.concat([X_data, y1_data, y2_data, y3_data, y4_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_data2 = pd.concat([X_data_unscale, y1_data, y2_data, y3_data, y4_data], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Sample data preparation\n",
    "x = del_data2['Nuclear']\n",
    "y = del_data2['PP1']\n",
    "z = del_data2['Total Cost']\n",
    "\n",
    "# Create grid values first.\n",
    "xi = np.linspace(x.min(), x.max(), 200)\n",
    "yi = np.linspace(y.min(), y.max(), 200)\n",
    "zi = griddata((x, y), z, (xi[None, :], yi[:, None]), method='cubic')\n",
    "\n",
    "# Create contour plot\n",
    "plt.contourf(xi, yi, zi, levels=30, cmap=\"RdBu_r\")\n",
    "plt.colorbar()\n",
    "#plt.scatter(x, y, c=z, edgecolors='k', linewidths=0.5)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel('Nuclear')\n",
    "plt.ylabel('PP')\n",
    "\n",
    "# Add a title (optional)\n",
    "#plt.title('Contour Plot of Total Cost')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============ Here\n",
    "#============\n",
    "#============\n",
    "#============\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# Sample data preparation\n",
    "x = del_data2['PP']\n",
    "y = del_data2['Nuclear']\n",
    "z = del_data2['CO2']\n",
    "\n",
    "# Create grid values first.\n",
    "xi = np.linspace(x.min(), x.max(), 200)\n",
    "yi = np.linspace(y.min(), y.max(), 200)\n",
    "zi = griddata((x, y), z, (xi[None, :], yi[:, None]), method='cubic')\n",
    "\n",
    "# Create contour plot\n",
    "plt.contourf(xi, yi, zi, levels=30, cmap=\"RdBu_r\")\n",
    "plt.colorbar()\n",
    "#plt.scatter(x, y, c=z, edgecolors='k', linewidths=0.5)\n",
    "\n",
    "# Add axis labels\n",
    "plt.xlabel('PP')\n",
    "plt.ylabel('Nuclear')\n",
    "\n",
    "# Add a title (optional)\n",
    "#plt.title('Contour Plot of CEEP')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'C': trial.suggest_float('C', 0.000001, 1000000),\n",
    "        'epsilon': trial.suggest_float('epsilon', 1e-6, 1e+1),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly']),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 0.00001, 0.1)\n",
    "    }\n",
    "\n",
    "    model = SVR(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n",
    "\n",
    "# Print the best value\n",
    "print('Best value', study.best_value)\n",
    "\n",
    "# Print the best trial\n",
    "print('Best trial', study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_param_y1 = study.best_params\n",
    "\n",
    "svr_model_y1 = SVR(**svr_param_y1)\n",
    "svr_model_y1.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = svr_model_y1.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = svr_model_y1.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = svr_model_y1.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y2_data\n",
    "y_train = y2_train\n",
    "y_test = y2_test\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'C': trial.suggest_float('C', 0.000001, 1000000),\n",
    "        'epsilon': trial.suggest_float('epsilon', 1e-6, 1e+1),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf']),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 0.00001, 0.1)\n",
    "    }\n",
    "\n",
    "    model = SVR(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_param_y2 = study.best_params\n",
    "\n",
    "svr_model_y2 = SVR(**svr_param_y2)\n",
    "svr_model_y2.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = svr_model_y2.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = svr_model_y2.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = svr_model_y2.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y3_data\n",
    "y_train = y3_train\n",
    "y_test = y3_test\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'C': trial.suggest_float('C', 0.000001, 1000000),\n",
    "        'epsilon': trial.suggest_float('epsilon', 1e-6, 1e+1),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf']),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 0.00001, 0.1)\n",
    "    }\n",
    "\n",
    "    model = SVR(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_param_y3 = study.best_params\n",
    "\n",
    "svr_model_y3 = SVR(**svr_param_y3)\n",
    "svr_model_y3.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = svr_model_y3.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = svr_model_y3.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = svr_model_y3.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y4_data\n",
    "y_train = y4_train\n",
    "y_test = y4_test\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'C': trial.suggest_float('C', 0.000001, 1000000),\n",
    "        'epsilon': trial.suggest_float('epsilon', 1e-6, 1e+1),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf']),\n",
    "        'degree': trial.suggest_int('degree', 1, 5),\n",
    "        'gamma': trial.suggest_float('gamma', 0.00001, 0.1)\n",
    "    }\n",
    "\n",
    "    model = SVR(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression', pruner=optuna.pruners.MedianPruner())\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_param_y4 = study.best_params\n",
    "\n",
    "svr_model_y4 = SVR(**svr_param_y4)\n",
    "svr_model_y4.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = svr_model_y4.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = svr_model_y4.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = svr_model_y4.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 1000)\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n",
    "\n",
    "# Print the best value\n",
    "print('Best value', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params_y1 = study.best_params\n",
    "\n",
    "xgb_model_y1 = XGBRegressor(**xgb_params_y1)\n",
    "xgb_model_y1.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb_model_y1.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = xgb_model_y1.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = xgb_model_y1.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y2_data\n",
    "y_train = y2_train\n",
    "y_test = y2_test\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 1000)\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n",
    "\n",
    "# Print the best value\n",
    "print('Best value', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params_y2 = study.best_params\n",
    "\n",
    "xgb_model_y2 = XGBRegressor(**xgb_params_y2)\n",
    "xgb_model_y2.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb_model_y2.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = xgb_model_y2.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = xgb_model_y2.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y3_data\n",
    "y_train = y3_train\n",
    "y_test = y3_test\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 1000)\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n",
    "\n",
    "# Print the best value\n",
    "print('Best value', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params_y3 = study.best_params\n",
    "\n",
    "xgb_model_y3 = XGBRegressor(**xgb_params_y3)\n",
    "xgb_model_y3.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb_model_y3.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = xgb_model_y3.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = xgb_model_y3.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y4_data\n",
    "y_train = y4_train\n",
    "y_test = y4_test\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 1.0),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0.01, 1.0),\n",
    "        'subsample': trial.suggest_float('subsample', 0.01, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.01, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.01, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 1.0),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 1000)\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n",
    "\n",
    "# Print the best value\n",
    "print('Best value', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params_y4 = study.best_params\n",
    "\n",
    "xgb_model_y4 = XGBRegressor(**xgb_params_y4)\n",
    "xgb_model_y4.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = xgb_model_y4.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = xgb_model_y4.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = xgb_model_y4.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "# Define the number of layers in the model\n",
    "n_layers = 4\n",
    "\n",
    "def objective(trial):\n",
    "    # Initialize an empty list to store layer sizes\n",
    "    layer_sizes = []\n",
    "    \n",
    "    # Add layer sizes to the list based on the number of layers\n",
    "    for i in range(n_layers):\n",
    "        layer_sizes.append(trial.suggest_int(f'layer_size_{i}', 1, 50))\n",
    "    \n",
    "    param = {\n",
    "        'hidden_layer_sizes': tuple(layer_sizes),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu']),\n",
    "        'solver': trial.suggest_categorical('solver', ['adam', 'lbfgs']),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 100),\n",
    "        'alpha': trial.suggest_float('alpha', 0.00001, 1),\n",
    "    }\n",
    "    model = MLPRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n",
    "\n",
    "# Print the best value\n",
    "print('Best value', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "ann_model_y1 = MLPRegressor(hidden_layer_sizes=(5, 5, 22, 46), activation='relu', alpha=0.6856023087910847, solver='lbfgs', random_state=88)\n",
    "\n",
    "# Fit the model on the training data\n",
    "ann_model_y1.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = ann_model_y1.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = ann_model_y1.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = ann_model_y1.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y2_data\n",
    "y_train = y2_train\n",
    "y_test = y2_test\n",
    "\n",
    "# Define the number of layers in the model\n",
    "n_layers = 4\n",
    "\n",
    "def objective(trial):\n",
    "    # Initialize an empty list to store layer sizes\n",
    "    layer_sizes = []\n",
    "    \n",
    "    # Add layer sizes to the list based on the number of layers\n",
    "    for i in range(n_layers):\n",
    "        layer_sizes.append(trial.suggest_int(f'layer_size_{i}', 1, 50))\n",
    "    \n",
    "    param = {\n",
    "        'hidden_layer_sizes': tuple(layer_sizes),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu']),\n",
    "        'solver': trial.suggest_categorical('solver', ['adam', 'lbfgs']),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 100),\n",
    "        'alpha': trial.suggest_float('alpha', 0.00001, 1),\n",
    "    }\n",
    "    model = MLPRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n",
    "\n",
    "# Print the best value\n",
    "print('Best value', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y2_data\n",
    "y_train = y2_train\n",
    "y_test = y2_test\n",
    "\n",
    "ann_model_y2 = MLPRegressor(hidden_layer_sizes=(33, 35, 12, 10), activation='relu', alpha=0.87732216286418, solver='lbfgs', random_state=58)\n",
    "\n",
    "# Fit the model on the training data\n",
    "ann_model_y2.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = ann_model_y2.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = ann_model_y2.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = ann_model_y2.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y3_data\n",
    "y_train = y3_train\n",
    "y_test = y3_test\n",
    "\n",
    "# Define the number of layers in the model\n",
    "n_layers = 4\n",
    "\n",
    "def objective(trial):\n",
    "    # Initialize an empty list to store layer sizes\n",
    "    layer_sizes = []\n",
    "    \n",
    "    # Add layer sizes to the list based on the number of layers\n",
    "    for i in range(n_layers):\n",
    "        layer_sizes.append(trial.suggest_int(f'layer_size_{i}', 1, 50))\n",
    "    \n",
    "    param = {\n",
    "        'hidden_layer_sizes': tuple(layer_sizes),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu']),\n",
    "        'solver': trial.suggest_categorical('solver', ['adam', 'lbfgs']),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 100),\n",
    "        'alpha': trial.suggest_float('alpha', 0.00001, 1),\n",
    "    }\n",
    "    model = MLPRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n",
    "\n",
    "# Print the best value\n",
    "print('Best value', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y3_data\n",
    "y_train = y3_train\n",
    "y_test = y3_test\n",
    "\n",
    "ann_model_y3 = MLPRegressor(hidden_layer_sizes=(4, 37, 41, 22), activation='relu', alpha=0.5394813016687909, solver='lbfgs', random_state=75)\n",
    "\n",
    "# Fit the model on the training data\n",
    "ann_model_y3.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = ann_model_y3.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = ann_model_y3.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = ann_model_y3.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y4_data\n",
    "y_train = y4_train\n",
    "y_test = y4_test\n",
    "\n",
    "# Define the number of layers in the model\n",
    "n_layers = 4\n",
    "\n",
    "def objective(trial):\n",
    "    # Initialize an empty list to store layer sizes\n",
    "    layer_sizes = []\n",
    "    \n",
    "    # Add layer sizes to the list based on the number of layers\n",
    "    for i in range(n_layers):\n",
    "        layer_sizes.append(trial.suggest_int(f'layer_size_{i}', 1, 50))\n",
    "    \n",
    "    param = {\n",
    "        'hidden_layer_sizes': tuple(layer_sizes),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu']),\n",
    "        'solver': trial.suggest_categorical('solver', ['adam', 'lbfgs']),\n",
    "        'random_state': trial.suggest_int('random_state', 1, 100),\n",
    "        'alpha': trial.suggest_float('alpha', 0.00001, 1),\n",
    "    }\n",
    "    model = MLPRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "# Create the study\n",
    "study = optuna.create_study(direction='maximize', study_name='regression')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Best parameters', study.best_params)\n",
    "\n",
    "# Print the best value\n",
    "print('Best value', study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = y4_data\n",
    "y_train = y4_train\n",
    "y_test = y4_test\n",
    "\n",
    "ann_model_y4 = MLPRegressor(hidden_layer_sizes=(8, 30, 46, 14), activation='relu', alpha=0.7138596741565628, solver='lbfgs', random_state=72)\n",
    "\n",
    "# Fit the model on the training data\n",
    "ann_model_y4.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = ann_model_y4.predict(X_train)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "\n",
    "y_pred = ann_model_y4.predict(X_test)\n",
    "test_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "y_data_pred = ann_model_y4.predict(X_data)\n",
    "data_r2 = r2_score(y_data, y_data_pred)\n",
    "\n",
    "print('Train R_sq:', train_r2)\n",
    "print('Test R_sq:', test_r2)\n",
    "print('Data R_sq:', data_r2)\n",
    "\n",
    "y_data_pred = pd.DataFrame(y_data_pred)\n",
    "y_data = pd.DataFrame(y_data)\n",
    "calculate_metrics(y_data, y_data_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_parallel_coordinate(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "# Model for y1\n",
    "model_y1 = SVR(C= 938266.6722022587, epsilon= 7.634680062462178, kernel= 'rbf', degree= 4, gamma= 0.00877730291334558)\n",
    "model_y1.fit(X_data, y_data)\n",
    "y1_pred = model_y1.predict(X_test)\n",
    "\n",
    "y_data = y2_data\n",
    "y_train = y2_train\n",
    "y_test = y2_test\n",
    "# Model for y2\n",
    "model_y2 = SVR(C= 71459.276928581, epsilon= 0.9031869098259523, kernel= 'rbf', degree= 1, gamma= 0.0035901390986543542)\n",
    "model_y2.fit(X_data, y_data)\n",
    "y2_pred = model_y2.predict(X_test)\n",
    "\n",
    "y_data = y3_data\n",
    "y_train = y3_train\n",
    "y_test = y3_test\n",
    "# Model for y3\n",
    "y3_params = {'max_depth': 5, 'learning_rate': 0.21192829865979532, 'n_estimators': 513, 'min_child_weight': 1, 'gamma': 0.8354449115144676, 'subsample': 0.8559761007878783, 'colsample_bytree': 0.9499990399914706, 'reg_alpha': 0.061420625912146976, 'reg_lambda': 0.9727616243556757, 'random_state': 599}\n",
    "model_y3 = XGBRegressor(**y3_params)\n",
    "model_y3.fit(X_data, y_data)\n",
    "y3_pred = model_y3.predict(X_test)\n",
    "\n",
    "y_data = y4_data\n",
    "y_train = y4_train\n",
    "y_test = y4_test\n",
    "# Model for y4\n",
    "y4_params = {'C': 435707.8208868281, 'epsilon': 0.25793769392540933, 'kernel': 'rbf', 'degree': 4, 'gamma': 0.0048393326352296085}\n",
    "model_y4 = MLPRegressor(hidden_layer_sizes=(7, 28, 23, 19), activation='relu', alpha=0.9365945996054417, solver='lbfgs', random_state=72)\n",
    "model_y4.fit(X_data, y_data)\n",
    "y4_pred = model_y4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Copy column names from df_source to df_target\n",
    "X_data.columns = X_data_unscale.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your list\n",
    "mm = [-0.08204877,  0.15594775, -0.00367291,  0.4247027,   0.84494347,  0.29067574,\n",
    "      -0.22331748,  0.815149,   -1.02562235, -0.44830675, -0.60883313, -1.03435034,\n",
    "      -0.12881452,  0.08031999, -0.78065315]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(mm)\n",
    "df = df.T\n",
    "df.columns = X_data_unscale.columns\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "model_y1 = SVR(C= 938266.6722022587, epsilon= 7.634680062462178, kernel= 'rbf', degree= 4, gamma= 0.00877730291334558)\n",
    "model_y1.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y1.predict, X_train)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "shap.plots.waterfall(shap_values[0], max_display=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data preparation (replace this with your actual data)\n",
    "# Assume del_data2, X_data, y1_data, y1_train, y1_test, and X_train are already defined\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the model\n",
    "model_y1 = SVR(C=938266.6722022587, epsilon=7.634680062462178, kernel='rbf', degree=4, gamma=0.00877730291334558)\n",
    "model_y1.fit(X_train_scaled, y1_train)\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y1.predict, X_train_scaled)\n",
    "shap_values = explainer.shap_values(X_train_scaled)\n",
    "\n",
    "# Create a DataFrame to map scaled and unscaled values\n",
    "X_train_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "X_train_scaled_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "unscaled_values = X_data_unscale.iloc[0]  # Example to use the first instance\n",
    "\n",
    "# Function to replace scaled feature values with unscaled values\n",
    "def replace_scaled_with_unscaled(shap_values, unscaled_values, feature_names):\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        shap_values.feature_names[i] = f\"{feature} (unscaled: {unscaled_values[feature]:.2f})\"\n",
    "    return shap_values\n",
    "\n",
    "# Create the SHAP waterfall plot\n",
    "shap_waterfall_plot = shap.plots.waterfall(shap_values[0], max_display=7)\n",
    "\n",
    "# Customize the SHAP plot to show unscaled values\n",
    "shap_values = replace_scaled_with_unscaled(shap_values, unscaled_values, X_data.columns)\n",
    "\n",
    "# Customize the axis labels using Matplotlib\n",
    "plt.xlabel(\"SHAP value (impact on model output)\")\n",
    "plt.ylabel(\"Optimum Capacities\")\n",
    "#plt.title(\"Customized SHAP Waterfall Plot with Unscaled Feature Values\")\n",
    "\n",
    "# Show the plot with custom labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "model_y1 = SVR(C= 938266.6722022587, epsilon= 7.634680062462178, kernel= 'rbf', degree= 4, gamma= 0.00877730291334558)\n",
    "model_y1.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y1.predict, X_train)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "#shap.plots.waterfall(shap_values[0], max_display=7)\n",
    "\n",
    "#=========================\n",
    "\n",
    "\n",
    "# Create a DataFrame to map scaled and unscaled values\n",
    "X_data_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "X_train_scaled_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "unscaled_values = X_data_df.iloc[0]  # Example to use the first instance\n",
    "\n",
    "# Function to replace scaled feature values with unscaled values\n",
    "def replace_scaled_with_unscaled(shap_values, unscaled_values, feature_names):\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        shap_values.feature_names[i] = f\"{feature}\"\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "MS = df\n",
    "s_mean = SS.mean_\n",
    "s_scale = SS.scale_\n",
    "Optimized_Inputs = (MS * s_scale) + s_mean\n",
    "Optimized_Inputs\n",
    "hk = Optimized_Inputs.to_numpy()\n",
    "hk\n",
    "shap_values.data = hk\n",
    "\n",
    "\n",
    "shap_waterfall_plot = shap.plots.waterfall(shap_values[0], max_display=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "model_y1 = SVR(C= 938266.6722022587, epsilon= 7.634680062462178, kernel= 'rbf', degree= 4, gamma= 0.00877730291334558)\n",
    "model_y1.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y1.predict, X_train)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "#shap.plots.waterfall(shap_values[0], max_display=7)\n",
    "\n",
    "#=========================\n",
    "\n",
    "\n",
    "# Create a DataFrame to map scaled and unscaled values\n",
    "X_data_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "X_train_scaled_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "unscaled_values = X_data_df.iloc[0]  # Example to use the first instance\n",
    "\n",
    "# Function to replace scaled feature values with unscaled values\n",
    "def replace_scaled_with_unscaled(shap_values, unscaled_values, feature_names):\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        shap_values.feature_names[i] = f\"{feature}\"\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "MS = df\n",
    "s_mean = SS.mean_\n",
    "s_scale = SS.scale_\n",
    "Optimized_Inputs = (MS * s_scale) + s_mean\n",
    "Optimized_Inputs\n",
    "hk = Optimized_Inputs.to_numpy()\n",
    "hk\n",
    "shap_values.data = hk\n",
    "\n",
    "\n",
    "shap_waterfall_plot = shap.plots.waterfall(shap_values[0], max_display=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "mod_1 = {'max_depth': 3, 'learning_rate': 0.011723269811235474, 'n_estimators': 875, 'min_child_weight': 6, 'gamma': 0.7712938662023604, 'subsample': 0.7899217905693013, 'colsample_bytree': 0.8285058920843318, 'reg_alpha': 0.6370415102229366, 'reg_lambda': 0.3197877984587, 'random_state': 560}\n",
    "\n",
    "model_y1 = XGBRegressor(**mod_1)\n",
    "model_y1.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.Explainer(model_y1, X_train)\n",
    "shap_values = explainer(X_data)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "shap.plots.beeswarm(shap_values, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y1_data\n",
    "y_train = y1_train\n",
    "y_test = y1_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "mod_1 = {'max_depth': 3, 'learning_rate': 0.011723269811235474, 'n_estimators': 875, 'min_child_weight': 6, 'gamma': 0.7712938662023604, 'subsample': 0.7899217905693013, 'colsample_bytree': 0.8285058920843318, 'reg_alpha': 0.6370415102229366, 'reg_lambda': 0.3197877984587, 'random_state': 560}\n",
    "\n",
    "model_y1 = XGBRegressor(**mod_1)\n",
    "model_y1.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.Explainer(model_y1, X_train)\n",
    "shap_values = explainer(X_data)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "shap.plots.beeswarm(shap_values, max_display=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y2_data\n",
    "y_train = y2_train\n",
    "y_test = y2_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "model_y2 = SVR(C= 71459.276928581, epsilon= 0.9031869098259523, kernel= 'rbf', degree= 1, gamma= 0.0035901390986543542)\n",
    "model_y2.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y2.predict, X_data)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "#shap.plots.waterfall(shap_values[0], max_display=20)\n",
    "\n",
    "\n",
    "#=========================\n",
    "\n",
    "\n",
    "# Create a DataFrame to map scaled and unscaled values\n",
    "X_data_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "X_train_scaled_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "unscaled_values = X_data_df.iloc[0]  # Example to use the first instance\n",
    "\n",
    "# Function to replace scaled feature values with unscaled values\n",
    "def replace_scaled_with_unscaled(shap_values, unscaled_values, feature_names):\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        shap_values.feature_names[i] = f\"{feature}\"\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "MS = df\n",
    "s_mean = SS.mean_\n",
    "s_scale = SS.scale_\n",
    "Optimized_Inputs = (MS * s_scale) + s_mean\n",
    "Optimized_Inputs\n",
    "hk = Optimized_Inputs.to_numpy()\n",
    "hk\n",
    "shap_values.data = hk\n",
    "\n",
    "\n",
    "shap_waterfall_plot = shap.plots.waterfall(shap_values[0], max_display=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y2_data\n",
    "y_train = y2_train\n",
    "y_test = y2_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "model_y2 = SVR(C= 71459.276928581, epsilon= 0.9031869098259523, kernel= 'rbf', degree= 1, gamma= 0.0035901390986543542)\n",
    "model_y2.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y2.predict, X_data)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "#shap.plots.waterfall(shap_values[0], max_display=20)\n",
    "\n",
    "\n",
    "#=========================\n",
    "\n",
    "\n",
    "# Create a DataFrame to map scaled and unscaled values\n",
    "X_data_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "X_train_scaled_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "unscaled_values = X_data_df.iloc[0]  # Example to use the first instance\n",
    "\n",
    "# Function to replace scaled feature values with unscaled values\n",
    "def replace_scaled_with_unscaled(shap_values, unscaled_values, feature_names):\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        shap_values.feature_names[i] = f\"{feature}\"\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "MS = df\n",
    "s_mean = SS.mean_\n",
    "s_scale = SS.scale_\n",
    "Optimized_Inputs = (MS * s_scale) + s_mean\n",
    "Optimized_Inputs\n",
    "hk = Optimized_Inputs.to_numpy()\n",
    "hk\n",
    "shap_values.data = hk\n",
    "\n",
    "\n",
    "shap_waterfall_plot = shap.plots.waterfall(shap_values[0], max_display=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y2_data\n",
    "y_train = y2_train\n",
    "y_test = y2_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "model_y2 = SVR(C= 71459.276928581, epsilon= 0.9031869098259523, kernel= 'rbf', degree= 1, gamma= 0.0035901390986543542)\n",
    "model_y2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y2.predict, X_data)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "shap.plots.waterfall(shap_values[0], max_display=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y2_data\n",
    "y_train = y2_train\n",
    "y_test = y2_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "mod_2 = {'max_depth': 3, 'learning_rate': 0.7866691318932307, 'n_estimators': 565, 'min_child_weight': 7, 'gamma': 0.9416813016248271, 'subsample': 0.9492401329994024, 'colsample_bytree': 0.9677794380528386, 'reg_alpha': 0.18569042405228248, 'reg_lambda': 0.8638844510641983, 'random_state': 666}\n",
    "\n",
    "model_y2 = XGBRegressor(**mod_2)\n",
    "model_y2.fit(X_data, y_data)\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.Explainer(model_y2, X_data)\n",
    "shap_values = explainer(X_data)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "shap.plots.beeswarm(shap_values,  max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y3_data\n",
    "y_train = y3_train\n",
    "y_test = y3_test\n",
    "\n",
    "# Import necessary libraries\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "# Define the parameters for the XGBRegressor\n",
    "y3_params = {\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.21192829865979532,\n",
    "    'n_estimators': 513,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.8354449115144676,\n",
    "    'subsample': 0.8559761007878783,\n",
    "    'colsample_bytree': 0.9499990399914706,\n",
    "    'reg_alpha': 0.061420625912146976,\n",
    "    'reg_lambda': 0.9727616243556757,\n",
    "    'random_state': 599\n",
    "}\n",
    "\n",
    "# Fit the model\n",
    "model_y3 = XGBRegressor(**y3_params)\n",
    "model_y3.fit(X_data, y_data)\n",
    "\n",
    "mm = [-0.08204877,  0.15594775, -0.00367291,  0.4247027,   0.84494347,  0.29067574,\n",
    " -0.22331748,  0.815149,   -1.02562235, -0.44830675, -0.60883313, -1.03435034,\n",
    " -0.12881452,  0.08031999, -0.78065315]\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.Explainer(model_y3, X_data)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "#shap.plots.waterfall(shap_values[0], max_display=20)\n",
    "\n",
    "\n",
    "\n",
    "#=========================\n",
    "\n",
    "\n",
    "# Create a DataFrame to map scaled and unscaled values\n",
    "X_data_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "X_train_scaled_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "unscaled_values = X_data_df.iloc[0]  # Example to use the first instance\n",
    "\n",
    "# Function to replace scaled feature values with unscaled values\n",
    "def replace_scaled_with_unscaled(shap_values, unscaled_values, feature_names):\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        shap_values.feature_names[i] = f\"{feature}\"\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "MS = df\n",
    "s_mean = SS.mean_\n",
    "s_scale = SS.scale_\n",
    "Optimized_Inputs = (MS * s_scale) + s_mean\n",
    "Optimized_Inputs\n",
    "hk = Optimized_Inputs.to_numpy()\n",
    "hk\n",
    "shap_values.data = hk\n",
    "\n",
    "\n",
    "shap_waterfall_plot = shap.plots.waterfall(shap_values[0], max_display=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y3_data\n",
    "y_train = y3_train\n",
    "y_test = y3_test\n",
    "\n",
    "# Import necessary libraries\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "# Define the parameters for the XGBRegressor\n",
    "y3_params = {\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.21192829865979532,\n",
    "    'n_estimators': 513,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.8354449115144676,\n",
    "    'subsample': 0.8559761007878783,\n",
    "    'colsample_bytree': 0.9499990399914706,\n",
    "    'reg_alpha': 0.061420625912146976,\n",
    "    'reg_lambda': 0.9727616243556757,\n",
    "    'random_state': 599\n",
    "}\n",
    "\n",
    "# Fit the model\n",
    "model_y3 = XGBRegressor(**y3_params)\n",
    "model_y3.fit(X_data, y_data)\n",
    "\n",
    "mm = [-0.08204877,  0.15594775, -0.00367291,  0.4247027,   0.84494347,  0.29067574,\n",
    " -0.22331748,  0.815149,   -1.02562235, -0.44830675, -0.60883313, -1.03435034,\n",
    " -0.12881452,  0.08031999, -0.78065315]\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.Explainer(model_y3, X_data)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "shap.plots.waterfall(shap_values[0], max_display=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y3_data\n",
    "y_train = y3_train\n",
    "y_test = y3_test\n",
    "\n",
    "# Import necessary libraries\n",
    "import shap\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "# Define the parameters for the XGBRegressor\n",
    "y3_params = {\n",
    "    'max_depth': 5,\n",
    "    'learning_rate': 0.21192829865979532,\n",
    "    'n_estimators': 513,\n",
    "    'min_child_weight': 1,\n",
    "    'gamma': 0.8354449115144676,\n",
    "    'subsample': 0.8559761007878783,\n",
    "    'colsample_bytree': 0.9499990399914706,\n",
    "    'reg_alpha': 0.061420625912146976,\n",
    "    'reg_lambda': 0.9727616243556757,\n",
    "    'random_state': 599\n",
    "}\n",
    "\n",
    "# Fit the model\n",
    "model_y3 = XGBRegressor(**y3_params)\n",
    "model_y3.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.Explainer(model_y3, X_data)\n",
    "shap_values = explainer(X_data)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "shap.plots.beeswarm(shap_values, max_display=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y4_data\n",
    "y_train = y4_train\n",
    "y_test = y4_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "y4_params = {'C': 435707.8208868281, 'epsilon': 0.25793769392540933, 'kernel': 'rbf', 'degree': 4, 'gamma': 0.0048393326352296085}\n",
    "model_y4 = MLPRegressor(hidden_layer_sizes=(7, 28, 23, 19), activation='relu', alpha=0.9365945996054417, solver='lbfgs', random_state=72)\n",
    "model_y4.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y4.predict, X_train)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "#shap.plots.waterfall(shap_values[0], max_display=20)\n",
    "\n",
    "#=========================\n",
    "\n",
    "\n",
    "# Create a DataFrame to map scaled and unscaled values\n",
    "X_data_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "X_train_scaled_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "unscaled_values = X_data_df.iloc[0]  # Example to use the first instance\n",
    "\n",
    "# Function to replace scaled feature values with unscaled values\n",
    "def replace_scaled_with_unscaled(shap_values, unscaled_values, feature_names):\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        shap_values.feature_names[i] = f\"{feature}\"\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "MS = df\n",
    "s_mean = SS.mean_\n",
    "s_scale = SS.scale_\n",
    "Optimized_Inputs = (MS * s_scale) + s_mean\n",
    "Optimized_Inputs\n",
    "hk = Optimized_Inputs.to_numpy()\n",
    "hk\n",
    "shap_values.data = hk\n",
    "\n",
    "\n",
    "shap_waterfall_plot = shap.plots.waterfall(shap_values[0], max_display=7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y4_data\n",
    "y_train = y4_train\n",
    "y_test = y4_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "y4_params = {'C': 435707.8208868281, 'epsilon': 0.25793769392540933, 'kernel': 'rbf', 'degree': 4, 'gamma': 0.0048393326352296085}\n",
    "model_y4 = MLPRegressor(hidden_layer_sizes=(7, 28, 23, 19), activation='relu', alpha=0.9365945996054417, solver='lbfgs', random_state=72)\n",
    "model_y4.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y4.predict, X_train)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "#shap.plots.waterfall(shap_values[0], max_display=20)\n",
    "\n",
    "#=========================\n",
    "\n",
    "\n",
    "# Create a DataFrame to map scaled and unscaled values\n",
    "X_data_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "X_train_scaled_df = pd.DataFrame(X_data, columns=X_data.columns)\n",
    "unscaled_values = X_data_df.iloc[0]  # Example to use the first instance\n",
    "\n",
    "# Function to replace scaled feature values with unscaled values\n",
    "def replace_scaled_with_unscaled(shap_values, unscaled_values, feature_names):\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        shap_values.feature_names[i] = f\"{feature}\"\n",
    "    return shap_values\n",
    "\n",
    "\n",
    "MS = df\n",
    "s_mean = SS.mean_\n",
    "s_scale = SS.scale_\n",
    "Optimized_Inputs = (MS * s_scale) + s_mean\n",
    "Optimized_Inputs\n",
    "hk = Optimized_Inputs.to_numpy()\n",
    "hk\n",
    "shap_values.data = hk\n",
    "\n",
    "\n",
    "shap_waterfall_plot = shap.plots.waterfall(shap_values[0], max_display=8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y4_data\n",
    "y_train = y4_train\n",
    "y_test = y4_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "y4_params = {'C': 435707.8208868281, 'epsilon': 0.25793769392540933, 'kernel': 'rbf', 'degree': 4, 'gamma': 0.0048393326352296085}\n",
    "model_y4 = MLPRegressor(hidden_layer_sizes=(7, 28, 23, 19), activation='relu', alpha=0.9365945996054417, solver='lbfgs', random_state=72)\n",
    "model_y4.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.KernelExplainer(model_y4.predict, X_train)\n",
    "shap_values = explainer(df)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "shap.plots.waterfall(shap_values[0], max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_data = y4_data\n",
    "y_train = y4_train\n",
    "y_test = y4_test\n",
    "\n",
    "# Ensure X_data is a DataFrame with proper feature names\n",
    "if not isinstance(X_data, pd.DataFrame):\n",
    "    X_data = pd.DataFrame(X_data)\n",
    "\n",
    "# Convert feature names to strings if they are not already\n",
    "X_data.columns = [str(col) for col in X_data.columns]\n",
    "\n",
    "\n",
    "mod_4 = {'max_depth': 10, 'learning_rate': 0.31195099027474943, 'n_estimators': 120, 'min_child_weight': 8, 'gamma': 0.5887647311991991, 'subsample': 0.8053950657222284, 'colsample_bytree': 0.9531753908252204, 'reg_alpha': 0.9543398120967802, 'reg_lambda': 0.05317709105394063, 'random_state': 693}\n",
    "\n",
    "model_y4 = XGBRegressor(**mod_4)\n",
    "model_y4.fit(X_data, y_data)\n",
    "\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.Explainer(model_y4, X_data)\n",
    "shap_values = explainer(X_data)\n",
    "\n",
    "# Plot the SHAP values using a waterfall plot\n",
    "shap.plots.beeswarm(shap_values, max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
